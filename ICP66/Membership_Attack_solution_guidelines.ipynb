{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Membership Attack - solution guidelines.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/veereshthotigar/CSEE5590-490-AI-CyberSecurity-/blob/master/ICP66/Membership_Attack_solution_guidelines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvK6LaVhrfZ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# required imports\n",
        "import sys \n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline  \n",
        "\n",
        "\n",
        "import torch\n",
        "import torchvision \n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data.sampler import SubsetRandomSampler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O05dkTKAsEh-",
        "colab_type": "code",
        "outputId": "e4f82285-e191-4a40-da75-f33eef9690e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# mount the google drive to download the datasets\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "project_path = '/content/drive/My Drive/cybersecurity'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEW1mLverl7l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create transforms to load the images, nothing much is needed here. \n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "# Normalize the test set same as training set without augmentation\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjsFJTFwr1wV",
        "colab_type": "code",
        "outputId": "afd7e94b-5d3b-40ce-8173-022744597bdc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# download CIFAR 10 training set\n",
        "trainset = torchvision.datasets.CIFAR10(root= project_path+'/data', train=True,\n",
        "                                        download=True, transform=transform_train)\n",
        "\n",
        "# load the trainning set\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True)\n",
        "\n",
        "# download the test data\n",
        "testset = torchvision.datasets.CIFAR10(root= project_path+'/data', train=False,\n",
        "                                        download=True, transform=transform_test)\n",
        "\n",
        "# load the test data\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=True)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "# check those manually on the dataset site: https://www.cs.toronto.edu/~kriz/cifar.html "
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itY7G_y3tDnC",
        "colab_type": "code",
        "outputId": "cf7cf79f-c086-431a-f8b5-d78cb94e7152",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "# helper function to unnormalize and plot image \n",
        "def imshow(img):\n",
        "    img = np.array(img)\n",
        "    img = img / 2 + 0.5\n",
        "    img = np.moveaxis(img, 0, -1)\n",
        "    plt.imshow(img)\n",
        "    \n",
        "# display sample from dataset \n",
        "imgs, labels = iter(trainloader).next()\n",
        "imshow(torchvision.utils.make_grid(imgs)) \n",
        "\n",
        "# notice who we converted the class idx to labels\n",
        "print(' '.join('%5s' % classes[labels[j]] for j in range(4)))\n",
        "\n",
        "# run this cell multiple times and notice diff images"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "plane  frog   dog   cat\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAB6CAYAAACvHqiXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJK32k42gbEK",
        "colab_type": "code",
        "outputId": "5edfea32-2e9a-4b67-fad7-c303bb6c03c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# divide the training dataset into the required groups Make sure they are balanced\n",
        "# original trainset is made of 50k images\n",
        "\n",
        "total_size = len(trainset)\n",
        "split1 = total_size // 4\n",
        "split2 = split1 * 2\n",
        "split3 = split1 * 3\n",
        "\n",
        "print(total_size, split1, split2, split3)\n",
        "\n",
        "indices = list(range(total_size))\n",
        "\n",
        "# two groups to train the shadow (in and out)\n",
        "shadow_train_idx = indices[:split1]\n",
        "shadow_out_idx = indices[split1:split2]\n",
        "\n",
        "# two groups to train the Target (in and out)\n",
        "target_train_idx = indices[split2:split3]\n",
        "target_out_idx =  indices[split3:]\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50000 12500 25000 37500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4wJ_0lkhp76",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 16# pick your own\n",
        "\n",
        "# divide and load shadow train in and out\n",
        "shadow_train_sampler = SubsetRandomSampler(shadow_train_idx) # Pytorch function\n",
        "shadow_train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, sampler=shadow_train_sampler)\n",
        "\n",
        "shadow_out_sampler = SubsetRandomSampler(shadow_out_idx) # Pytorch function\n",
        "shadow_out_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, sampler=shadow_out_sampler)\n",
        "\n",
        "# divide and load Target in and out\n",
        "target_train_sampler = SubsetRandomSampler(target_train_idx) # Pytorch function\n",
        "target_train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, sampler=target_train_sampler)\n",
        "\n",
        "target_out_sampler = SubsetRandomSampler(target_out_idx) # Pytorch function\n",
        "target_out_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, sampler=target_out_sampler)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kj9LuQJNuCXC",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "# create a CNN\n",
        "# Input shape (3, 32, 32) \n",
        "# architecture: simple. 2 conv and 2 Max pool, followed by 2 fc (120, 84) \n",
        "# output of fc is 10 because we have 10 classes!\n",
        "\n",
        "\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    \"\"\"CNN.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"CNN Builder.\"\"\"\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        self.conv_layer = nn.Sequential(\n",
        "\n",
        "            # Conv Layer block 1\n",
        "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            # Conv Layer block 2\n",
        "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Dropout2d(p=0.05),\n",
        "\n",
        "            # Conv Layer block 3\n",
        "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "\n",
        "\n",
        "        self.fc_layer = nn.Sequential(\n",
        "            nn.Dropout(p=0.1),\n",
        "            nn.Linear(4096, 1024),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=0.1),\n",
        "            nn.Linear(512, 10),\n",
        "            nn.LogSoftmax(dim=1)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Perform forward.\"\"\"\n",
        "        \n",
        "        # conv layers\n",
        "        x = self.conv_layer(x)\n",
        "        \n",
        "        # flatten\n",
        "        x = x.view(x.size(0), -1)\n",
        "        \n",
        "        # fc layer\n",
        "        x = self.fc_layer(x)\n",
        "\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlOhiXBHLV0H",
        "colab_type": "code",
        "outputId": "d9f6819c-696b-4aa6-c712-b2d85cb94ca3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# check if CUDA available or not\n",
        "print(torch.cuda.is_available())\n",
        "print(torch.cuda.get_device_name(0))\n",
        "\n",
        "# clear the cache\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ER_B4V8YjKyU",
        "colab_type": "code",
        "outputId": "02af3c44-e06c-4700-fdab-614431ca656e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# initalize a target model and train it\n",
        "\n",
        "target_model = CNN()\n",
        "taget_model = target_model.cuda()\n",
        "criterion = nn.CrossEntropyLoss() # CrossEntropyLoss\n",
        "optimizer = optim.Adam(target_model.parameters(), lr=0.0003) # try Adam VS SGD\n",
        "\n",
        "    \n",
        "epochs = 20\n",
        "for e in range(epochs):\n",
        "    running_loss = 0\n",
        "    for images, labels in target_train_loader:\n",
        "        # sending tensors to GPU\n",
        "        images = images.cuda()\n",
        "        labels = labels.cuda()\n",
        "        optimizer.zero_grad()\n",
        "        logits = target_model(images)\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        \n",
        "        running_loss += loss.item()\n",
        "    else:\n",
        "        print(\"\\nEpoch : {}/{}..\".format(e+1,epochs),f\"Training loss: {running_loss/len(target_train_loader)}\")\n",
        "\n",
        "#save the model\n",
        "print(\"Model: \\n\\n\", target_model, '\\n')\n",
        "torch.save(target_model.state_dict(), project_path+'/target_checkpoint.pth')"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch : 1/20.. Training loss: 1.8490159610653167\n",
            "\n",
            "Epoch : 2/20.. Training loss: 1.491386187122301\n",
            "\n",
            "Epoch : 3/20.. Training loss: 1.3183889469069898\n",
            "\n",
            "Epoch : 4/20.. Training loss: 1.1910944996435013\n",
            "\n",
            "Epoch : 5/20.. Training loss: 1.0917825206085239\n",
            "\n",
            "Epoch : 6/20.. Training loss: 1.0097852958666393\n",
            "\n",
            "Epoch : 7/20.. Training loss: 0.9311403772791328\n",
            "\n",
            "Epoch : 8/20.. Training loss: 0.8763449314953117\n",
            "\n",
            "Epoch : 9/20.. Training loss: 0.8231527041215116\n",
            "\n",
            "Epoch : 10/20.. Training loss: 0.7706686467542063\n",
            "\n",
            "Epoch : 11/20.. Training loss: 0.729572062354411\n",
            "\n",
            "Epoch : 12/20.. Training loss: 0.6951780501667344\n",
            "\n",
            "Epoch : 13/20.. Training loss: 0.6492430977046947\n",
            "\n",
            "Epoch : 14/20.. Training loss: 0.6262647310424301\n",
            "\n",
            "Epoch : 15/20.. Training loss: 0.6065641323109264\n",
            "\n",
            "Epoch : 16/20.. Training loss: 0.5742024364964584\n",
            "\n",
            "Epoch : 17/20.. Training loss: 0.5497367937365533\n",
            "\n",
            "Epoch : 18/20.. Training loss: 0.5260654520529234\n",
            "\n",
            "Epoch : 19/20.. Training loss: 0.5119597138860799\n",
            "\n",
            "Epoch : 20/20.. Training loss: 0.48427774930072714\n",
            "Model: \n",
            "\n",
            " CNN(\n",
            "  (conv_layer): Sequential(\n",
            "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace)\n",
            "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (4): ReLU(inplace)\n",
            "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (8): ReLU(inplace)\n",
            "    (9): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (10): ReLU(inplace)\n",
            "    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (12): Dropout2d(p=0.05)\n",
            "    (13): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (14): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (15): ReLU(inplace)\n",
            "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (17): ReLU(inplace)\n",
            "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (fc_layer): Sequential(\n",
            "    (0): Dropout(p=0.1)\n",
            "    (1): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "    (2): ReLU(inplace)\n",
            "    (3): Linear(in_features=1024, out_features=512, bias=True)\n",
            "    (4): ReLU(inplace)\n",
            "    (5): Dropout(p=0.1)\n",
            "    (6): Linear(in_features=512, out_features=10, bias=True)\n",
            "    (7): LogSoftmax()\n",
            "  )\n",
            ") \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpY8ktdskRQN",
        "colab_type": "code",
        "outputId": "cc982ec2-2f37-42c4-d7de-42e6d655b161",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# calculate the accuracy of the Target Model\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in target_out_loader:\n",
        "        # sending tensors to GPU\n",
        "        images = images.cuda()\n",
        "        labels = labels.cuda()\n",
        "        outputs = target_model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the 12500 test images: %d %%' % (100 * correct / total))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the 12500 test images: 76 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEClKFqikmUl",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ha_VqRVVkoCm",
        "colab_type": "code",
        "outputId": "764c8c18-fc61-4abe-84c9-12e4f53fabb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# initalize a Shadow Model and Train it\n",
        "# for the first ICP, your shadow model can have the same CNN architecture and hyperparameters\n",
        "\n",
        "shadow_model = CNN()\n",
        "# clear the cache\n",
        "torch.cuda.empty_cache()\n",
        "#send to GPU\n",
        "shadow_model = shadow_model.cuda()\n",
        "shadow_criterion =  nn.CrossEntropyLoss() # CrossEntropyLoss\n",
        "shadow_optimizer = optim.Adam(shadow_model.parameters(), lr=0.0003) # ADAM \n",
        "\n",
        "\n",
        "# let the magic begin\n",
        "epochs = 20\n",
        "with torch.set_grad_enabled(True):\n",
        "  for e in range(epochs):\n",
        "      running_loss = 0\n",
        "      for images, labels in shadow_train_loader:\n",
        "          # sending tensors to GPU\n",
        "          images = images.cuda()\n",
        "          labels = labels.cuda()\n",
        "          shadow_optimizer.zero_grad()\n",
        "          logits = shadow_model(images)\n",
        "          shadow_loss = shadow_criterion(logits, labels)\n",
        "          shadow_loss.backward()\n",
        "          shadow_optimizer.step()\n",
        "\n",
        "\n",
        "          running_loss += shadow_loss.item()\n",
        "      else:\n",
        "          print(\"\\nEpoch : {}/{}..\".format(e+1,epochs),f\"Training loss: {running_loss/len(shadow_train_loader)}\")\n",
        "\n",
        "#save the model\n",
        "print(\"Our model: \\n\\n\", shadow_model, '\\n')\n",
        "torch.save(shadow_model.state_dict(), project_path+'/shadow_checkpoint.pth')\n",
        "print('Finished Training the Shadow model')"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch : 1/20.. Training loss: 1.795286073709083\n",
            "\n",
            "Epoch : 2/20.. Training loss: 1.4833759107553135\n",
            "\n",
            "Epoch : 3/20.. Training loss: 1.2908298326727679\n",
            "\n",
            "Epoch : 4/20.. Training loss: 1.1557366430302105\n",
            "\n",
            "Epoch : 5/20.. Training loss: 1.0460994323653638\n",
            "\n",
            "Epoch : 6/20.. Training loss: 0.965517583481796\n",
            "\n",
            "Epoch : 7/20.. Training loss: 0.9019076326847686\n",
            "\n",
            "Epoch : 8/20.. Training loss: 0.8414648659241474\n",
            "\n",
            "Epoch : 9/20.. Training loss: 0.7911420002617799\n",
            "\n",
            "Epoch : 10/20.. Training loss: 0.7442171693496082\n",
            "\n",
            "Epoch : 11/20.. Training loss: 0.7087334181798999\n",
            "\n",
            "Epoch : 12/20.. Training loss: 0.6759818431151949\n",
            "\n",
            "Epoch : 13/20.. Training loss: 0.6344128431528425\n",
            "\n",
            "Epoch : 14/20.. Training loss: 0.6001081456956656\n",
            "\n",
            "Epoch : 15/20.. Training loss: 0.5770573489787176\n",
            "\n",
            "Epoch : 16/20.. Training loss: 0.5506062763826469\n",
            "\n",
            "Epoch : 17/20.. Training loss: 0.5148120153781093\n",
            "\n",
            "Epoch : 18/20.. Training loss: 0.5068512069218604\n",
            "\n",
            "Epoch : 19/20.. Training loss: 0.47778532800772\n",
            "\n",
            "Epoch : 20/20.. Training loss: 0.459841628789978\n",
            "Our model: \n",
            "\n",
            " CNN(\n",
            "  (conv_layer): Sequential(\n",
            "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace)\n",
            "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (4): ReLU(inplace)\n",
            "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (8): ReLU(inplace)\n",
            "    (9): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (10): ReLU(inplace)\n",
            "    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (12): Dropout2d(p=0.05)\n",
            "    (13): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (14): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (15): ReLU(inplace)\n",
            "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (17): ReLU(inplace)\n",
            "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (fc_layer): Sequential(\n",
            "    (0): Dropout(p=0.1)\n",
            "    (1): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "    (2): ReLU(inplace)\n",
            "    (3): Linear(in_features=1024, out_features=512, bias=True)\n",
            "    (4): ReLU(inplace)\n",
            "    (5): Dropout(p=0.1)\n",
            "    (6): Linear(in_features=512, out_features=10, bias=True)\n",
            "    (7): LogSoftmax()\n",
            "  )\n",
            ") \n",
            "\n",
            "Finished Training the Shadow model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHZNbnkgRCZ0",
        "colab_type": "code",
        "outputId": "60745fe4-dab6-4afa-a498-b462665620d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# calculate the accuracy of the Shadow Model\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in shadow_out_loader:\n",
        "        # sending tensors to GPU\n",
        "        images = images.cuda()\n",
        "        labels = labels.cuda()\n",
        "        outputs = shadow_model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        #print(predicted)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the 12500 test images: %d %%' % (100 * correct / total))"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the 12500 test images: 67 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmnk_ZVJWPF0",
        "colab_type": "text"
      },
      "source": [
        "Got the same accuracy as target model since I have used same architecture and hyperparameters !"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocvC5oilXnXB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_checkpoint(filepath):\n",
        "    checkpoint = torch.load(filepath)\n",
        "    model = CNN()\n",
        "    model.load_state_dict(checkpoint)\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAIYt1goU0VS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 1# pick your own\n",
        "\n",
        "# divide and load shadow train in and out\n",
        "shadow_train_sampler = SubsetRandomSampler(shadow_train_idx) # Pytorch function\n",
        "shadow_train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, sampler=shadow_train_sampler)\n",
        "\n",
        "shadow_out_sampler = SubsetRandomSampler(shadow_out_idx) # Pytorch function\n",
        "shadow_out_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, sampler=shadow_out_sampler)\n",
        "\n",
        "# divide and load Target in and out\n",
        "target_train_sampler = SubsetRandomSampler(target_train_idx) # Pytorch function\n",
        "target_train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, sampler=target_train_sampler)\n",
        "\n",
        "target_out_sampler = SubsetRandomSampler(target_out_idx) # Pytorch function\n",
        "target_out_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, sampler=target_out_sampler)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0kP_-62ljFE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "714bbe38-af93-4e17-f691-c6ef95b33449"
      },
      "source": [
        "#load the model\n",
        "load_shadow_model = load_checkpoint(project_path+'/shadow_checkpoint.pth')\n",
        "load_shadow_model = load_shadow_model.cuda()\n",
        "\n",
        "# freeze the Shadow model \n",
        "for param in load_shadow_model.parameters():\n",
        "    param.requires_grad = False\n",
        "    \n",
        "# make predictions on both datasets (shadow_in and shdow_out)\n",
        "predictions = []\n",
        "label_size = (1,1)\n",
        "input_size = (1,10)\n",
        "\n",
        "labels_0 = np.zeros(label_size,  dtype=float)\n",
        "labels_1 = np.ones(label_size,  dtype=float)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in shadow_train_loader:\n",
        "        # sending tensors to GPU\n",
        "        images = images.cuda()\n",
        "        labels = labels.cuda()\n",
        "        logps = load_shadow_model(images)\n",
        "        ps = torch.exp(logps) \n",
        "        ps = ps.cpu()\n",
        "        predictions.append([ps.data.numpy(),labels_1])   \n",
        "with torch.no_grad():\n",
        "    for images, labels in shadow_out_loader:\n",
        "        # sending tensors to GPU\n",
        "        images = images.cuda()\n",
        "        labels = labels.cuda()\n",
        "        logps = load_shadow_model(images)\n",
        "        ps = torch.exp(logps) \n",
        "        ps = ps.cpu()\n",
        "        predictions.append([ps.data.numpy(),labels_0]) \n",
        "        \n",
        "print(predictions[0])  \n",
        "print(predictions[13000]) \n"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[array([[3.2274544e-04, 2.7303781e-06, 5.9128026e-05, 5.4613398e-03,\n",
            "        2.1700147e-03, 2.0722309e-03, 2.2249265e-05, 9.8978853e-01,\n",
            "        3.6974970e-06, 9.7308686e-05]], dtype=float32), array([[1.]])]\n",
            "[array([[8.3442954e-03, 7.4253714e-04, 5.0414698e-03, 3.9414432e-02,\n",
            "        2.4616938e-05, 1.1276531e-03, 1.5451212e-04, 5.9461374e-05,\n",
            "        9.7436838e-05, 9.4499356e-01]], dtype=float32), array([[0.]])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFRCL63ZqkRE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a new dataset of the shape [predictions(shadow_in), 1], [predicitons(shadow_out), 0] and zip them together\n",
        "#save the dataset\n",
        "import pickle\n",
        "\n",
        "with open(project_path+'/data/shadow.data', 'wb') as filehandle:\n",
        "    pickle.dump(predictions, filehandle)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWQFEqXXZK7l",
        "colab_type": "code",
        "outputId": "88221953-4e2b-40eb-98c1-a378daffc97a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "import pickle\n",
        "#load the dataset\n",
        "with open(project_path+'/data/shadow.data', 'rb') as filehandle:\n",
        "    # read the data as binary data stream\n",
        "    predictionsList = pickle.load(filehandle)\n",
        "    \n",
        "print(predictionsList[0])  \n",
        "print(predictionsList[13000]) "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[array([[3.2274544e-04, 2.7303781e-06, 5.9128026e-05, 5.4613398e-03,\n",
            "        2.1700147e-03, 2.0722309e-03, 2.2249265e-05, 9.8978853e-01,\n",
            "        3.6974970e-06, 9.7308686e-05]], dtype=float32), array([[1.]])]\n",
            "[array([[8.3442954e-03, 7.4253714e-04, 5.0414698e-03, 3.9414432e-02,\n",
            "        2.4616938e-05, 1.1276531e-03, 1.5451212e-04, 5.9461374e-05,\n",
            "        9.7436838e-05, 9.4499356e-01]], dtype=float32), array([[0.]])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ymKnj7QpdDG",
        "colab_type": "code",
        "outputId": "dc09d331-27d6-44f7-c6bb-850c2b2c2e6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# create the Attack Model: A NN binary classifier {0, 1}\n",
        "# the input to this model is the propability distribution vector of size 10\n",
        "# and the output is either 0 (input was not included in training) or 1\n",
        "from torch.autograd import Variable\n",
        "attack_model = nn.Sequential(nn.Linear(10, 20),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(20, 26),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(26, 16),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(16, 8),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(8, 2),\n",
        "                      nn.LogSoftmax(dim=1))\n",
        "attack_model = attack_model.cuda()\n",
        "attack_criterion = nn.CrossEntropyLoss()\n",
        "attack_optimizer = optim.Adam(attack_model.parameters(), lr=0.001)\n",
        "\n",
        "epochs = 5\n",
        "for e in range(epochs):\n",
        "    running_loss = 0\n",
        "    for images, labels in predictionsList:\n",
        "        # sending tensors to GPU\n",
        "        images = torch.from_numpy(images).cuda().float()\n",
        "        labels = torch.from_numpy(labels).cuda().long()\n",
        "        attack_optimizer.zero_grad()\n",
        "        output = attack_model(images)\n",
        "        #output = torch.argmax(output,dim=1)\n",
        "        #print(labels.view(-1),output)\n",
        "        loss = attack_criterion(output, labels.view(-1))\n",
        "        loss.backward()\n",
        "        attack_optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "    else:\n",
        "        print(f\"Training loss: {running_loss/len(predictionsList)}\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training loss: 0.012167028627970762\n",
            "Training loss: 0.03292711387530793\n",
            "Training loss: 0.02034705390798548\n",
            "Training loss: 0.023673287838757398\n",
            "Training loss: 0.018494805540799422\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0J1B1OhMp6er",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# calculate the recall and precision of your attack network using the Target_out and Target_in datasets\n",
        "# to do so, take a random numer of datapoints, run them throw the target model,\n",
        "#load the model\n",
        "load_target_model = load_checkpoint(project_path+'/target_checkpoint.pth')\n",
        "load_target_model = load_target_model.cuda()\n",
        "\n",
        "# freeze the Shadow model \n",
        "for param in load_target_model.parameters():\n",
        "    param.requires_grad = False\n",
        "    \n",
        "# make predictions on both datasets (shadow_in and shdow_out)\n",
        "predictions = []\n",
        "label_size = (1,1)\n",
        "\n",
        "labels_0 = np.zeros(label_size,  dtype=float)\n",
        "labels_1 = np.ones(label_size,  dtype=float)\n",
        "with torch.no_grad():\n",
        "    for images, labels in shadow_train_loader:\n",
        "        # sending tensors to GPU\n",
        "        images = images.cuda()\n",
        "        labels = labels.cuda()\n",
        "        logps = load_target_model(images)\n",
        "        ps = torch.exp(logps) \n",
        "        ps = ps.cpu()\n",
        "        predictions.append([ps.data.numpy(),labels_1])   \n",
        "with torch.no_grad():\n",
        "    for images, labels in shadow_out_loader:\n",
        "        # sending tensors to GPU\n",
        "        images = images.cuda()\n",
        "        labels = labels.cuda()\n",
        "        logps = load_target_model(images)\n",
        "        ps = torch.exp(logps) \n",
        "        ps = ps.cpu()\n",
        "        predictions.append([ps.data.numpy(),labels_0]) \n",
        "        \n",
        "#save the dataset\n",
        "import pickle\n",
        "\n",
        "with open(project_path+'/data/target.data', 'wb') as filehandle:\n",
        "    pickle.dump(predictions, filehandle)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffqsLQIn5ru_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load the dataset\n",
        "with open(project_path+'/data/target.data', 'rb') as filehandle:\n",
        "    # read the data as binary data stream\n",
        "    testloader = pickle.load(filehandle)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVLxc0lY5RYo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "955484cc-ea73-4919-daaf-51c3028228c5"
      },
      "source": [
        "# and then input the output of the target model to your attack network \n",
        "# you already know the target_in and target_out samples, so use that info to evaluate the attack model\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for input_, labels in predictionsList:\n",
        "        # sending tensors to GPU\n",
        "        input_ = torch.from_numpy(input_).cuda().float()\n",
        "        labels = torch.from_numpy(labels).cuda().long()\n",
        "        outputs = attack_model(input_)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels.long()).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the 12500 test images: %d %%' % (\n",
        "    100 * correct / total))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the 12500 test images: 50 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMsyCgziqeaa",
        "colab_type": "text"
      },
      "source": [
        "Great! At this point, you must have created a succesfful attack model that can detect whether a datapoint was used in training a target mode or not. \n",
        "* A successful attack model is one with a precision/recall higher than 85% -- you are using same architecture and are aware of the data classes\n",
        "\n",
        " \n",
        " Can you suggest any defense mechanism? If yes, Apply them to your solution and re-evaluate your attack model. How did your defense mecanism affect the accuracy of the target model? How did it affect the recall and precision of the Attack model?"
      ]
    }
  ]
}