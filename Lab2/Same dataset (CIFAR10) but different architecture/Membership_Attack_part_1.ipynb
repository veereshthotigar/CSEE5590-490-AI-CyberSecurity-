{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Membership Attack - part-1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/veereshthotigar/CSEE5590-490-AI-CyberSecurity-/blob/master/Lab2/Same%20dataset%20(CIFAR10)%20but%20different%20architecture/Membership_Attack_part_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvK6LaVhrfZ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# required imports\n",
        "import sys \n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline  \n",
        "\n",
        "\n",
        "import torch\n",
        "import torchvision \n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data.sampler import SubsetRandomSampler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O05dkTKAsEh-",
        "colab_type": "code",
        "outputId": "5f224da9-757f-4e2a-ae26-40621edbbd2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "# mount the google drive to download the datasets\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "project_path = '/content/drive/My Drive/cybersecurity/Lab-Assignment-2/Part-2'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEW1mLverl7l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create transforms to load the images, nothing much is needed here. \n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "# Normalize the test set same as training set without augmentation\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjsFJTFwr1wV",
        "colab_type": "code",
        "outputId": "00736e92-05d7-4d74-bd31-978c9de387b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# download CIFAR 10 training set\n",
        "trainset = torchvision.datasets.CIFAR10(root= project_path+'/data', train=True,\n",
        "                                        download=True, transform=transform_train)\n",
        "\n",
        "# load the trainning set\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True)\n",
        "\n",
        "# download the test data\n",
        "testset = torchvision.datasets.CIFAR10(root= project_path+'/data', train=False,\n",
        "                                        download=True, transform=transform_test)\n",
        "\n",
        "# load the test data\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=True)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "# check those manually on the dataset site: https://www.cs.toronto.edu/~kriz/cifar.html "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itY7G_y3tDnC",
        "colab_type": "code",
        "outputId": "524e5781-cc6d-4cb8-d51f-80de203e9209",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "# helper function to unnormalize and plot image \n",
        "def imshow(img):\n",
        "    img = np.array(img)\n",
        "    img = img / 2 + 0.5\n",
        "    img = np.moveaxis(img, 0, -1)\n",
        "    plt.imshow(img)\n",
        "    \n",
        "# display sample from dataset \n",
        "imgs, labels = iter(trainloader).next()\n",
        "imshow(torchvision.utils.make_grid(imgs)) \n",
        "\n",
        "# notice who we converted the class idx to labels\n",
        "print(' '.join('%5s' % classes[labels[j]] for j in range(4)))\n",
        "\n",
        "# run this cell multiple times and notice diff images"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "horse  deer  deer plane\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAB6CAYAAACvHqiXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmCTwL3QPDw+PSQL/Qvfw8PCYJPj/3l/V62PobGAAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJK32k42gbEK",
        "colab_type": "code",
        "outputId": "c102d832-a380-440e-b549-d3f2db43c5fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# divide the training dataset into the required groups Make sure they are balanced\n",
        "# original trainset is made of 50k images\n",
        "\n",
        "total_size = len(trainset)\n",
        "split1 = total_size // 4\n",
        "split2 = split1 * 2\n",
        "split3 = split1 * 3\n",
        "\n",
        "print(total_size, split1, split2, split3)\n",
        "\n",
        "indices = list(range(total_size))\n",
        "\n",
        "# two groups to train the shadow (in and out)\n",
        "shadow_train_idx = indices[:split1]\n",
        "shadow_out_idx = indices[split1:split2]\n",
        "\n",
        "# two groups to train the Target (in and out)\n",
        "target_train_idx = indices[split2:split3]\n",
        "target_out_idx =  indices[split3:]\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50000 12500 25000 37500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4wJ_0lkhp76",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 10# pick your own\n",
        "\n",
        "# divide and load shadow train in and out\n",
        "shadow_train_sampler = SubsetRandomSampler(shadow_train_idx) # Pytorch function\n",
        "shadow_train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, sampler=shadow_train_sampler)\n",
        "\n",
        "shadow_out_sampler = SubsetRandomSampler(shadow_out_idx) # Pytorch function\n",
        "shadow_out_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, sampler=shadow_out_sampler)\n",
        "\n",
        "# divide and load Target in and out\n",
        "target_train_sampler = SubsetRandomSampler(target_train_idx) # Pytorch function\n",
        "target_train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, sampler=target_train_sampler)\n",
        "\n",
        "target_out_sampler = SubsetRandomSampler(target_out_idx) # Pytorch function\n",
        "target_out_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, sampler=target_out_sampler)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2m0DwoK5Xt0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_checkpoint(filepath):\n",
        "    checkpoint = torch.load(filepath)\n",
        "    model = CNN()\n",
        "    model.load_state_dict(checkpoint)\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YejeqA68933D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CNN(nn.Module):\n",
        "    \"\"\"CNN.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"CNN Builder.\"\"\"\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        self.conv_layer = nn.Sequential(\n",
        "\n",
        "            # Conv Layer block 1\n",
        "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            # Conv Layer block 2\n",
        "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Dropout2d(p=0.05),\n",
        "\n",
        "            # Conv Layer block 3\n",
        "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "\n",
        "\n",
        "        self.fc_layer = nn.Sequential(\n",
        "            nn.Dropout(p=0.1),\n",
        "            nn.Linear(4096, 1024),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=0.1),\n",
        "            nn.Linear(512, 10),\n",
        "            nn.LogSoftmax(dim=1)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Perform forward.\"\"\"\n",
        "        \n",
        "        # conv layers\n",
        "        x = self.conv_layer(x)\n",
        "        \n",
        "        # flatten\n",
        "        x = x.view(x.size(0), -1)\n",
        "        \n",
        "        # fc layer\n",
        "        x = self.fc_layer(x)\n",
        "\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yAlGMwn5f78",
        "colab_type": "text"
      },
      "source": [
        "Below is the preTrained Target Model loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbLKKajm5eUZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load the model\n",
        "load_target_model = load_checkpoint(project_path+'/../../target_checkpoint.pth')\n",
        "load_target_model = load_target_model.cuda()\n",
        "\n",
        "# freeze the Target model \n",
        "for param in load_target_model.parameters():\n",
        "    param.requires_grad = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RD6Yc_R44QCX",
        "colab_type": "text"
      },
      "source": [
        "Below is Shadow Model Architecture different from target model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygSJAmC94UjK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "# Shadow Model Architecture\n",
        "# Input shape (3, 32, 32) \n",
        "# architecture: simple. 2 conv and 2 Max pool, followed by 2 fc (120, 84) \n",
        "# output of fc is 10 because we have 10 classes!\n",
        "\n",
        "\n",
        "\n",
        "class shadow(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(shadow, self).__init__()\n",
        "\n",
        "        self.fc_layer = nn.Sequential(\n",
        "            nn.Linear(3*32*32, 2304),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(2304, 1536),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(1536, 1024),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=0.1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(1024, 768),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(768, 512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=0.1),\n",
        "            nn.Linear(512, 10),\n",
        "            nn.LogSoftmax(dim=1)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Perform forward.\"\"\"\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        # fc layer\n",
        "        x = self.fc_layer(x)\n",
        "\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlOhiXBHLV0H",
        "colab_type": "code",
        "outputId": "199e92d1-9f58-4e74-eda7-d706d04099a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# check if CUDA available or not\n",
        "print(torch.cuda.is_available())\n",
        "print(torch.cuda.get_device_name(0))\n",
        "# clear the cache\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ha_VqRVVkoCm",
        "colab_type": "code",
        "outputId": "af770329-8fdd-4fef-902d-9ac0f1a77275",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# initalize a Shadow Model and Train it\n",
        "shadow_model = shadow()\n",
        "# clear the cache\n",
        "torch.cuda.empty_cache()\n",
        "#send to GPU\n",
        "shadow_model = shadow_model.cuda()\n",
        "shadow_criterion =  nn.CrossEntropyLoss() # CrossEntropyLoss\n",
        "shadow_optimizer = optim.SGD(shadow_model.parameters(), lr=0.001) # SGD \n",
        "# let the magic begin\n",
        "epochs = 20\n",
        "with torch.set_grad_enabled(True):\n",
        "  for e in range(epochs):\n",
        "      running_loss = 0\n",
        "      for images, labels in shadow_train_loader:\n",
        "          # sending tensors to GPU\n",
        "          images = images.cuda()\n",
        "          labels = labels.cuda()\n",
        "          shadow_optimizer.zero_grad()\n",
        "          logits = shadow_model(images)\n",
        "          shadow_loss = shadow_criterion(logits, labels)\n",
        "          shadow_loss.backward()\n",
        "          shadow_optimizer.step()\n",
        "\n",
        "\n",
        "          running_loss += shadow_loss.item()\n",
        "      else:\n",
        "          print(\"\\nEpoch : {}/{}..\".format(e+1,epochs),f\"Training loss: {running_loss/len(shadow_train_loader)}\")\n",
        "\n",
        "#save the model\n",
        "print(\"Our model: \\n\\n\", shadow_model, '\\n')\n",
        "torch.save(shadow_model.state_dict(), project_path+'/shadow_checkpoint.pth')\n",
        "print('Finished Training the Shadow model')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch : 1/20.. Training loss: 2.3021997776031493\n",
            "\n",
            "Epoch : 2/20.. Training loss: 2.2993014295578003\n",
            "\n",
            "Epoch : 3/20.. Training loss: 2.295217490005493\n",
            "\n",
            "Epoch : 4/20.. Training loss: 2.2881613527297975\n",
            "\n",
            "Epoch : 5/20.. Training loss: 2.271895202255249\n",
            "\n",
            "Epoch : 6/20.. Training loss: 2.2317551002502443\n",
            "\n",
            "Epoch : 7/20.. Training loss: 2.1594572323799133\n",
            "\n",
            "Epoch : 8/20.. Training loss: 2.0930086928367615\n",
            "\n",
            "Epoch : 9/20.. Training loss: 2.0530957913398744\n",
            "\n",
            "Epoch : 10/20.. Training loss: 2.0281572698593138\n",
            "\n",
            "Epoch : 11/20.. Training loss: 2.0064695532798766\n",
            "\n",
            "Epoch : 12/20.. Training loss: 1.9861209619522096\n",
            "\n",
            "Epoch : 13/20.. Training loss: 1.9675459579467773\n",
            "\n",
            "Epoch : 14/20.. Training loss: 1.9463089265823363\n",
            "\n",
            "Epoch : 15/20.. Training loss: 1.9284512697219849\n",
            "\n",
            "Epoch : 16/20.. Training loss: 1.9057356109619141\n",
            "\n",
            "Epoch : 17/20.. Training loss: 1.8796040097236633\n",
            "\n",
            "Epoch : 18/20.. Training loss: 1.8651349051475525\n",
            "\n",
            "Epoch : 19/20.. Training loss: 1.8411611392974854\n",
            "\n",
            "Epoch : 20/20.. Training loss: 1.8203672404289246\n",
            "Our model: \n",
            "\n",
            " shadow(\n",
            "  (fc_layer): Sequential(\n",
            "    (0): Linear(in_features=3072, out_features=2304, bias=True)\n",
            "    (1): ReLU(inplace)\n",
            "    (2): Linear(in_features=2304, out_features=1536, bias=True)\n",
            "    (3): ReLU(inplace)\n",
            "    (4): Linear(in_features=1536, out_features=1024, bias=True)\n",
            "    (5): ReLU(inplace)\n",
            "    (6): Dropout(p=0.1)\n",
            "    (7): ReLU(inplace)\n",
            "    (8): Linear(in_features=1024, out_features=768, bias=True)\n",
            "    (9): ReLU(inplace)\n",
            "    (10): Linear(in_features=768, out_features=512, bias=True)\n",
            "    (11): ReLU(inplace)\n",
            "    (12): Dropout(p=0.1)\n",
            "    (13): Linear(in_features=512, out_features=10, bias=True)\n",
            "    (14): LogSoftmax()\n",
            "  )\n",
            ") \n",
            "\n",
            "Finished Training the Shadow model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHZNbnkgRCZ0",
        "colab_type": "code",
        "outputId": "8be6c4b1-aa3f-4ec5-a518-a0d09cb28e5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# calculate the accuracy of the Shadow Model\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in shadow_out_loader:\n",
        "        # sending tensors to GPU\n",
        "        images = images.cuda()\n",
        "        labels = labels.cuda()\n",
        "        outputs = shadow_model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        #print(predicted)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the {len(shadow_out_loader)*batch_size} test images: %d %%' % (100 * correct / total))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the 12500 test images: 32 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAIYt1goU0VS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#creating dataset for attack model\n",
        "batch_size = 1\n",
        "\n",
        "# divide and load shadow train in and out\n",
        "shadow_train_sampler = SubsetRandomSampler(shadow_train_idx) # Pytorch function\n",
        "shadow_train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, sampler=shadow_train_sampler)\n",
        "\n",
        "shadow_out_sampler = SubsetRandomSampler(shadow_out_idx) # Pytorch function\n",
        "shadow_out_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, sampler=shadow_out_sampler)\n",
        "\n",
        "# divide and load Target in and out\n",
        "target_train_sampler = SubsetRandomSampler(target_train_idx) # Pytorch function\n",
        "target_train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, sampler=target_train_sampler)\n",
        "\n",
        "target_out_sampler = SubsetRandomSampler(target_out_idx) # Pytorch function\n",
        "target_out_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, sampler=target_out_sampler)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jH9WehgalVJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_checkpoint(filepath):\n",
        "    checkpoint = torch.load(filepath)\n",
        "    model = shadow()\n",
        "    model.load_state_dict(checkpoint)\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0kP_-62ljFE",
        "colab_type": "code",
        "outputId": "b25d76b1-b8d5-483e-94fd-67ba89a4f019",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "#load the model\n",
        "load_shadow_model = load_checkpoint(project_path+'/shadow_checkpoint.pth')\n",
        "load_shadow_model = load_shadow_model.cuda()\n",
        "\n",
        "# freeze the Shadow model \n",
        "for param in load_shadow_model.parameters():\n",
        "    param.requires_grad = False\n",
        "    \n",
        "# make predictions on both datasets (shadow_in and shdow_out)\n",
        "predictions = []\n",
        "\n",
        "labels_0 = 0#np.zeros(1)\n",
        "labels_1 = 1#np.ones(1)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in shadow_train_loader:\n",
        "        # sending tensors to GPU\n",
        "        images = images.cuda()\n",
        "        labels = labels.cuda()\n",
        "        logps = load_shadow_model(images)\n",
        "        ps = torch.exp(logps) \n",
        "        ps = ps.cpu()\n",
        "        pred = ps.data.numpy()\n",
        "        predictions.append([pred[0],labels_1])   \n",
        "with torch.no_grad():\n",
        "    for images, labels in shadow_out_loader:\n",
        "        # sending tensors to GPU\n",
        "        images = images.cuda()\n",
        "        labels = labels.cuda()\n",
        "        logps = load_shadow_model(images)\n",
        "        ps = torch.exp(logps) \n",
        "        ps = ps.cpu()\n",
        "        pred = ps.data.numpy()\n",
        "        predictions.append([pred[0],labels_0]) \n",
        "        \n",
        "print(predictions[0])  \n",
        "print(predictions[13000]) \n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[array([0.02651548, 0.36778998, 0.0254264 , 0.14886226, 0.03306021,\n",
            "       0.09916725, 0.10686831, 0.04934694, 0.04831476, 0.09464866],\n",
            "      dtype=float32), 1]\n",
            "[array([0.0733829 , 0.12254832, 0.04213513, 0.08741152, 0.05218967,\n",
            "       0.04098951, 0.02806151, 0.22130057, 0.06222088, 0.2697599 ],\n",
            "      dtype=float32), 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFRCL63ZqkRE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a new dataset of the shape [predictions(shadow_in), 1], [predicitons(shadow_out), 0] and zip them together\n",
        "#save the dataset\n",
        "import pickle\n",
        "\n",
        "with open(project_path+'/data/shadow.data', 'wb') as filehandle:\n",
        "    pickle.dump(predictions, filehandle)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0J1B1OhMp6er",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# calculate the recall and precision of your attack network using the Target_out and Target_in datasets\n",
        "# to do so, take a random numer of datapoints, run them throw the target model,\n",
        "    \n",
        "# make predictions on both datasets (target_in and target_out)\n",
        "predictions = []\n",
        "label_size = (1,1)\n",
        "\n",
        "labels_0 = 0\n",
        "labels_1 = 1\n",
        "with torch.no_grad():\n",
        "    for images, labels in target_train_loader:\n",
        "        # sending tensors to GPU\n",
        "        images = images.cuda()\n",
        "        labels = labels.cuda()\n",
        "        logps = load_target_model(images)\n",
        "        ps = torch.exp(logps) \n",
        "        ps = ps.cpu()\n",
        "        pred = ps.data.numpy()\n",
        "        predictions.append([pred[0],labels_1])   \n",
        "with torch.no_grad():\n",
        "    for images, labels in target_out_loader:\n",
        "        # sending tensors to GPU\n",
        "        images = images.cuda()\n",
        "        labels = labels.cuda()\n",
        "        logps = load_target_model(images)\n",
        "        ps = torch.exp(logps) \n",
        "        ps = ps.cpu()\n",
        "        pred = ps.data.numpy()\n",
        "        predictions.append([pred[0],labels_0]) \n",
        "        \n",
        "#save the dataset\n",
        "import pickle\n",
        "\n",
        "with open(project_path+'/data/target.data', 'wb') as filehandle:\n",
        "    pickle.dump(predictions, filehandle)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMsyCgziqeaa",
        "colab_type": "text"
      },
      "source": [
        "Continued in part - 2"
      ]
    }
  ]
}